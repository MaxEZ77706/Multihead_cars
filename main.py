# ============================================
# batch-inference ONNX c imgsz + WARMUP + –ü–†–ï–î–ü–†–û–°–ú–û–¢–† (–∫–∞–∫ –≤ –ø–µ—Ä–≤–æ–º –∫–æ–¥–µ)
# ============================================

import cv2
import numpy as np
import time
import onnxruntime as ort
from ultralytics import YOLO
import multiprocessing
import torch

# === PERFORMANCE BOOST ===
cv2.setNumThreads(0)  # –∏–Ω–æ–≥–¥–∞ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ FPS
torch.set_num_threads(min(8, multiprocessing.cpu_count()))
torch.backends.cudnn.benchmark = True

# === CLASS LABELS ===
brand_classes = ["Motorcycle", "SUV", "Sedan", "Truck", "Van"]
color_classes = ["Black", "Blue", "Gray", "Red", "White"]

# === NORMALIZATION VALUES ===
mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)
std  = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)

# === Box shrink (–¥–µ–ª–∞–µ–º —Ä–∞–º–∫–∏ –ø–æ–º–µ–Ω—å—à–µ) ===
BOX_SHRINK = 0.12  # 12% —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã; –ø–æ—Å—Ç–∞–≤—å 0.0 –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ

def inset_box(x1, y1, x2, y2, shrink, W, H):
    w = x2 - x1
    h = y2 - y1
    dx = int(w * shrink)
    dy = int(h * shrink)
    nx1 = max(0, x1 + dx)
    ny1 = max(0, y1 + dy)
    nx2 = min(W - 1, x2 - dx)
    ny2 = min(H - 1, y2 - dy)
    if nx2 <= nx1 or ny2 <= ny1:
        return x1, y1, x2, y2
    return nx1, ny1, nx2, ny2

# === Load ONNX Multihead Model ===
providers = ["CPUExecutionProvider"]  # –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –º–æ–∂–Ω–æ –ø—Ä–æ–±–æ–≤–∞—Ç—å CoreMLExecutionProvider / CUDAExecutionProvider
multihead_sess = ort.InferenceSession("models/model N6/multihead(batching).onnx", providers=providers)
input_name = multihead_sess.get_inputs()[0].name

# === Warmup ONNX Multihead Model ===
dummy = np.random.randn(1, 3, 224, 224).astype(np.float32)
_ = multihead_sess.run(None, {input_name: dummy})
print("[onnx] Warmup done.")

# === Load YOLOv8 Detector ===
object_detector = YOLO("yolov8n.pt")

# === VIDEO SETUP ===
video_path = "my_cars1.mp4"
cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")

fps = cap.get(cv2.CAP_PROP_FPS)
if not fps or fps <= 1e-3:
    fps = 25.0  # –∑–∞–ø–∞—Å–Ω–æ–π FPS, –µ—Å–ª–∏ –Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è

width, height = 640, 416
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter("output_multihead_batch.mp4", fourcc, fps, (width, height))
if not out.isOpened():
    # –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –∫–æ–¥–µ–∫–∞
    fourcc = cv2.VideoWriter_fourcc(*'avc1')
    out = cv2.VideoWriter("output_multihead_batch.mp4", fourcc, fps, (width, height))
    if not out.isOpened():
        raise RuntimeError("VideoWriter –Ω–µ –æ—Ç–∫—Ä—ã–ª—Å—è. –ü–æ–ø—Ä–æ–±—É–π –∫–æ–¥–µ–∫ 'XVID' –∏–ª–∏ –¥—Ä—É–≥–æ–π –ø—É—Ç—å/—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ.")

# === –æ–∫–Ω–æ –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ø–µ—Ä–≤–æ–º –∫–æ–¥–µ) ===
win_name = "Real-time Detection"
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)
cv2.resizeWindow(win_name, 960, 540)

frame_id = 0
total_start_time = time.time()

while cap.isOpened():
    start_time = time.time()
    success, frame = cap.read()
    if not success:
        break

    frame_id += 1
    frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_LINEAR)

    # === YOLO DETECTION ===
    with torch.inference_mode():
        results = object_detector(frame, conf=0.4, imgsz=224)
    boxes = results[0].boxes.xyxy.int().tolist() if results and results[0].boxes is not None else []

    crops, coords = [], []
    for x1, y1, x2, y2 in boxes:
        # –∫–ª–∏–ø–ø–∏–Ω–≥ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü—ã –∫–∞–¥—Ä–∞
        x1 = max(0, min(width - 1, x1)); y1 = max(0, min(height - 1, y1))
        x2 = max(0, min(width - 1, x2)); y2 = max(0, min(height - 1, y2))
        if x2 <= x1 or y2 <= y1:
            continue

        # —É–º–µ–Ω—å—à–∞–µ–º –±–æ–∫—Å –≤–Ω—É—Ç—Ä—å
        x1, y1, x2, y2 = inset_box(x1, y1, x2, y2, BOX_SHRINK, width, height)

        car_crop = frame[y1:y2, x1:x2]
        if car_crop.size == 0 or car_crop.shape[0] < 10 or car_crop.shape[1] < 10:
            continue

        car_resized = cv2.resize(car_crop, (224, 224), interpolation=cv2.INTER_LINEAR)
        car_rgb = cv2.cvtColor(car_resized, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
        img = (car_rgb.transpose(2, 0, 1) - mean) / std  # CHW
        crops.append(img.astype(np.float32))
        coords.append((x1, y1, x2, y2))

    if crops:
        batch = np.stack(crops, axis=0).astype(np.float32)  # [B,3,224,224]
        brand_out, color_out = multihead_sess.run(None, {input_name: batch})

        for i, (x1, y1, x2, y2) in enumerate(coords):
            brand_pred = int(np.argmax(brand_out[i]))
            color_pred = int(np.argmax(color_out[i]))
            label = f"{brand_classes[brand_pred]}, {color_classes[color_pred]}"
            # –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∏–µ —Ä–∞–º–∫–∏ –∏ —à—Ä–∏—Ñ—Ç
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)
            cv2.putText(frame, label, (x1, max(y1 - 8, 0)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)

    # === –∑–∞–ø–∏—Å—å –∫–∞–¥—Ä–∞ ===
    out.write(frame)

    # === –ü–û–ö–ê–ó –ö–ê–î–†–ê (–∫–∞–∫ –≤ –ø–µ—Ä–≤–æ–º –∫–æ–¥–µ) ===
    preview = cv2.resize(frame, None, fx=0.6, fy=0.6, interpolation=cv2.INTER_LINEAR)
    cv2.imshow(win_name, preview)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        print("[ui] –í—ã—Ö–æ–¥ –ø–æ 'q'.")
        break

    # –ª–æ–≥ –ø–æ –≤—Ä–µ–º–µ–Ω–∏/—Å–∫–æ—Ä–æ—Å—Ç–∏
    elapsed = time.time() - start_time
    fps_inst = 1.0 / elapsed if elapsed > 1e-6 else 0.0
    print(f"‚úÖ Frame {frame_id} in {elapsed:.2f}s | ‚ö° {fps_inst:.2f} FPS")

# === RELEASE ===
cap.release()
out.release()
cv2.destroyAllWindows()

# === FINAL STATS ===
total_elapsed = time.time() - total_start_time
print(f"\nüßÆ Total frames: {frame_id}")
print(f"‚è±Ô∏è Total time: {total_elapsed:.2f} sec")
print(f"‚ö° Avg FPS: {frame_id / total_elapsed:.2f}")
print("üéâ Done: output_multihead_batch.mp4 saved.")
